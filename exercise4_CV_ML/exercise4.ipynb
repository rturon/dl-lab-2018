{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will learn how Bayesian optimization and Hyperband works and how we can combine them to optimize the hyperparameters of a convolutional neural network (CNN) on CIFAR-10. The architecture consists of 3 convolutional layers (with RELU activations and batch norm) and one final fully-connected layer. We use Adam to optimize the weights of the network.\n",
    "\n",
    "\n",
    "Before you start, make sure that you installed the following dependencies:\n",
    "- numpy (pip install numpy)\n",
    "- scipy (pip install scipy)\n",
    "- sklearn (pip install sklearn==0.18.1) Note: The random forest surrogate was trained with sklearn version  0.18.1 . Newer versions should also work but probably generate some warnings and in future might lead to slightly different results.\n",
    "- emukit (pip install git+https://github.com/amzn/Emukit.git)\n",
    "- statsmodel (pip install git+git://github.com/statsmodels/statsmodels.git) you might also need cython for that (pip install cython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid spending too much time training CNNs, we will not optimize the original benchmark but instead optimize a so-called surrogate benchmark. A surrogate benchmark is basically just a regression model (in our case a random forest) that was trained on a large set of randomly sampled hyperparameter configurations of the original benchmark (CNNs in our case).\n",
    "Now to evaluate a hyperparameter configuration, we do not actually train the CNN but use the prediction of the regression model, which is much faster to evaluate (miliseconds). We can do that not only for the validation error but also for the training time of the CNN. Obviously, our surrogate is only an approximation of the true benchmark, however just for developing new hyperparameter optimization methods it should be sufficient. You can download the surrogates [here](http://www.ml4aad.org/wp-content/uploads/2018/12/surrogates_cnn.tar.gz).\n",
    "\n",
    "As in the previous exercises, please hand in a 1 to 2 pages report about what you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "rf = pickle.load(open(\"./rf_surrogate_cnn.pkl\", \"rb\"))\n",
    "cost_rf = pickle.load(open(\"./rf_cost_surrogate_cnn.pkl\", \"rb\"))\n",
    "\n",
    "def objective_function(x, epoch=40):\n",
    "    \"\"\"\n",
    "    Function wrapper to approximate the validation error after N epochs of the hyperparameter configurations\n",
    "    x by the prediction of a surrogate regression model, which was trained on the validation error of \n",
    "    randomly sampled hyperparameter configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize all hyperparameter to be in [0, 1]\n",
    "    x_norm = deepcopy(x)\n",
    "    x_norm[0] = (x[0] - (-6)) / (0 - (-6))\n",
    "    x_norm[1] = (x[1] - 32) / (512 - 32)\n",
    "    x_norm[2] = (x[2] - 4) / (10 - 4)\n",
    "    x_norm[3] = (x[3] - 4) / (10 - 4)\n",
    "    x_norm[4] = (x[4] - 4) / (10 - 4)\n",
    "    \n",
    "\n",
    "    x_norm = np.append(x_norm, epoch)\n",
    "    y = rf.predict(x_norm[None, :])[0]\n",
    "\n",
    "    return y\n",
    "\n",
    "def runtime(x, epoch=40):\n",
    "    \"\"\"\n",
    "    Function wrapper to approximate the training time of the hyperparameter configurations x.\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize all hyperparameter to be in [0, 1]\n",
    "    x_norm = deepcopy(x)\n",
    "    x_norm[0] = (x[0] - (-6)) / (0 - (-6))\n",
    "    x_norm[1] = (x[1] - 32) / (512 - 32)\n",
    "    x_norm[2] = (x[2] - 4) / (10 - 4)\n",
    "    x_norm[3] = (x[3] - 4) / (10 - 4)\n",
    "    x_norm[4] = (x[4] - 4) / (10 - 4)\n",
    "    \n",
    "\n",
    "    x_norm = np.append(x_norm, epoch)\n",
    "    y = cost_rf.predict(x_norm[None, :])[0]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [emukit](https://github.com/amzn/emukit) for the Bayesian optimization part of this exercise. Have a look on [the emukit tutorial](https://github.com/amzn/emukit/blob/develop/notebooks/Emukit-tutorial-Bayesian-optimization-introduction.ipynb) for Bayesian optimization to get familiar with it.\n",
    "\n",
    "The first thing we need to do is to define the configuration space for our CNN surrogate benchmark. We will optimize the learning rate, batch size and the number of filters in each of the 3 convolutional layers and for convenience treat all hyperparamters as continuous variables. For the learning rate it is usually good practice to optimize it on a logarithmic scale. Because of that we optimize the exponents in $log\\alpha \\in [-6, -1]$ and then in the objective function set the learning rate to $\\alpha = 10^{log\\alpha}$ (this is handled by our surrogate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ContinuousParameter, ParameterSpace\n",
    "\n",
    "list_params = []\n",
    "list_params.append(ContinuousParameter(\"learning_rate\",-6, -1))\n",
    "list_params.append(ContinuousParameter(\"batch_size\",32, 512))\n",
    "list_params.append(ContinuousParameter(\"n_filters_1\", 16, 1024))\n",
    "list_params.append(ContinuousParameter(\"n_filters_2\", 16, 1024))\n",
    "list_params.append(ContinuousParameter(\"n_filters_3\", 16, 1024))\n",
    "\n",
    "space = ParameterSpace(list_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned in the lecture, Bayesian optimization internally uses a model to guide the search. However, in order to train a model we first need data. Thus, we will first use a so called initial design to collect some data points before we start with the actual Bayesian optimization loop. Here we will simply draw and evaluate N random configurations. However, one could do more sophisticated things such as experimental design or multi-task learning to warmstart the optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.experimental_design.model_free.random_design import RandomDesign\n",
    "\n",
    "n_init = 2\n",
    "\n",
    "# TODO: draw n_init configurations randomly \n",
    "\n",
    "# TODO: evaluate all configurations and store them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the main loop, we have to define the ingredients that are needed for Bayesian optimization:\n",
    "* a Gaussian process as a probabilistic model for the objective function\n",
    "* Expected improvement as the acquisition function\n",
    "* an optimizer for the acquistion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyModelWrapper\n",
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement\n",
    "from emukit.core.optimization import AcquisitionOptimizer\n",
    "\n",
    "kernel = GPy.kern.Matern52(X_init.shape[1], ARD=True)\n",
    "gp = GPyModelWrapper(GPy.models.GPRegression(X_init, Y_init, kernel, noise_var=1e-10))\n",
    "ei = ExpectedImprovement(gp)\n",
    "optimizer = AcquisitionOptimizer(space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the main loop which 1. optimizes the acquisition function, 2. evaluates the objective function 3. augments our dataset and updates our model.\n",
    "Additionally we will also keep track of the current best solution we have found (dubbed incumbent) which BO will ultimately return to the user. Again, there are differnt possible ways to identify the incumbent. Here, we will simply use the best observed configuration, but for instance one could also optimize the posterior mean.\n",
    "\n",
    "To estimate how much time it would have taken to evaluate the real instead of the surrogate benchmark, we also predict the training time of each hyperparameter configuration and plot the incumbent performance over the cumulative runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 20\n",
    "\n",
    "incumbent_trajectory = []  # keeps track of the incumbent after each iterations\n",
    "runtime_trajectory = []   # saves the runtime after each interation\n",
    "t = 0\n",
    "current_incumbent = 1\n",
    "for i in range(n_iters):\n",
    "    \n",
    "    # TODO: optimize acquisition function\n",
    "    \n",
    "    # TODO: evaluate objective function and estimate the training time\n",
    "    \n",
    "    # TODO: augment data with new observations and update the model\n",
    "    \n",
    "    # TODO: bookkeeping of the incumbent and the runtime\n",
    "    \n",
    "plt.plot(runtime_trajectory, incumbent_trajectory)\n",
    "\n",
    "plt.xlabel(\"estimated runtime (seconds)\")\n",
    "plt.ylabel(\"validation error\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Bayesian Optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperband"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we saw that we often have access to cheap approximations of the objective function, called fidelities. Here, we will use the learning curve, i e. the validation error after each epoch, as fidelity to speed up the optimization process.\n",
    "This part of the exercise is inspired from Kevin Jamieson's [tutorial](http://people.eecs.berkeley.edu/~kjamieson/hyperband.html). I highly recommend to check it out to gain a better understanding of Hyperband and successive halving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of maintaining a model as in Bayesian optimization, Hyperband samples configurations randomly. Thus, the first bit we are going to implement is a function that returns a random hyperparameter configuration sampled from a uniform distribution over the same configuration space as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_hyperparameter_configuration():\n",
    "    # TODO: draw and return a random configuation \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successive halving is a simple bandit strategy to allocate resources to a fixed set of configurations.\n",
    "Given a set of $N$ configurations, it ranks them based on their performance on the minimum budget and continues only the top $\\eta^{-1}$ configurations on a budget which is $\\eta$ times larger.\n",
    "\n",
    "Hyperbands combines random search with successive halving to balance very aggressive evaluation with many configurations on the smallest budget and very conservative runs on the maximum budget.\n",
    "For that, in each iteration, called bracket, of its outer loop, Hyperband runs successive halving with alternating number of configurations and budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_halving(n, r, s):\n",
    "    \"\"\"\n",
    "    Run one iteration of successive halving\n",
    "    :param n: initial number of hyperparameter configurations\n",
    "    :param r: initial number of iterations \n",
    "    :param s: defines the tradeoff between r and n\n",
    "    \"\"\"\n",
    "    T = [ get_random_hyperparameter_configuration() for i in range(n) ] \n",
    "    for i in range(s+1):\n",
    "        val_losses = []\n",
    "        # TODO compute the number of configs n_i and epochs r_i\n",
    "        \n",
    "        # TODO run all n_i configuration for r_i epochs \n",
    "            \n",
    "        plt.scatter(np.ones(len(val_losses)) * r_i, val_losses)\n",
    "            \n",
    "        # TODO: keep only the best n_i/eta configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 40  # maximum iterations/epochs per configuration\n",
    "eta = 3 # defines downsampling rate (default=3)\n",
    "s_max = int(np.log(max_iter)/np.log(eta))  # number of unique executions of Successive Halving (minus one)\n",
    "B = (s_max+1)*max_iter  # total number of iterations (without reuse) per execution of Succesive Halving (n,r)\n",
    "\n",
    "\n",
    "for s in reversed(range(s_max+1)):\n",
    "    \n",
    "    # TODO: compute initial number of configurations and initial number to run configurations for\n",
    "\n",
    "    # run successive halving\n",
    "    successive_halving(n, r, s)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"validation error\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"successive halving\")\n",
    "    plt.xlim(0, 41)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Bayesian optimization with Hyperband"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the weaknesses of Hyperband is that it draws configurations randomly and hence might take exponentially long to approach the global optimum. In the last part of the exercise, we will combine Hyperband with a kernel density estimator that models the distribution of the good and the bad configurations in the input space. By sampling from this model instead of a uniform distribution we can find good configurations much faster.\n",
    "\n",
    "To implement the kernel density estimator we will use [statsmodel](http://www.statsmodels.org/stable/index.html). In the original implementation of the tree parzen estimator [TPE](http://jaberg.github.io/hyperopt/), they fitted a univariate kernel density estimator for each dimension independently. Even though this allows to quickly train a model it is not able to capture interactions between hyperparameters. Instead, we will use a [multivariate kernel density estimator](http://www.statsmodels.org/stable/generated/statsmodels.nonparametric.kernel_density.KDEMultivariate.html#statsmodels.nonparametric.kernel_density.KDEMultivariate) that fits a distribution over the whole input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import scipy.stats as sps\n",
    "\n",
    "def sample(num_samples=64, random_fraction=1/3, min_bandwidth=1e-3):\n",
    "    \"\"\"\n",
    "    Samples one configuration either randomly or by optimizing expected improvement\n",
    "    \n",
    "    :param num_samples: number of samples to optimize expected improvement\n",
    "    :param random_fraction: fraction of configuration that will be sampled randomly\n",
    "    :param min_bandwith: minimum allowed bandwith for the kernel density estimator\n",
    "\n",
    "    \n",
    "    :return: hyperparameter configuration, boolean flag that indicated whether the configuration was sampled based on the model or not\n",
    "    \"\"\"\n",
    "    # TODO: if no model is available, sample randomly also mix in a fraction (random_fraction) of random configs\n",
    "    \n",
    "    # TODO: estimate the largest budget for which we have a model\n",
    "    \n",
    "    # define EI acquisition function\n",
    "    l = kde_models[budget]['good'].pdf\n",
    "    g = kde_models[budget]['bad' ].pdf\n",
    "    negative_ei = lambda x: max(1e-32, g(x))/max(l(x),1e-32)  # The acquisition function is always maximized, but since we minimze here we put a minus in fron\n",
    "\n",
    "    kde_good = kde_models[budget]['good']\n",
    "    kde_bad = kde_models[budget]['bad']\n",
    "\n",
    "    # draw num_samples samples\n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        # TODO: sample random training data point from the good configurations\n",
    "\n",
    "        # TODO: take the training data point as the mean and the KDE bandwidth as std and sample from a multivariate Normal\n",
    "\n",
    "        # TODO: compute the EI acquisition function\n",
    "            \n",
    "    # TODO: return the best sample with the highest EI value\n",
    "    return best_sample, True\n",
    "\n",
    "\n",
    "def update(budget, x, y, min_points_in_model=5, top_n_percent=15, min_bandwidth=1e-3):\n",
    "    \"\"\"\n",
    "    Add new observed x, y to the already collected data on this budget and\n",
    "    if possible updates the kernel density estimator for the corresponding budget.\n",
    "    \n",
    "    :param budget: on which budget the hyperparameter configuration was observed\n",
    "    :param x: the new observed hyperparameter configuration\n",
    "    :param y: the corresponding loss\n",
    "    :param min_points_in_model: the minimum number of datapoint that are need to fit a KDE\n",
    "    :param top_n_percent: specifies how many percent of the best configurations will be used to train the KDE for the good configurations\n",
    "    :param min_bandwith: minimum allowed bandwith for the kernel density estimator\n",
    "    \"\"\"\n",
    "    # check if we have already data points for this budget collected\n",
    "    if budget not in configs.keys():\n",
    "        configs[budget] = []\n",
    "        losses[budget] = []\n",
    "\n",
    "    # store configuration and loss\n",
    "    configs[budget].append(x)\n",
    "    losses[budget].append(y)\n",
    "\n",
    "    # skip model building if not enough points are available\n",
    "    if len(configs[budget]) <= min_points_in_model-1:\n",
    "        return\n",
    "\n",
    "    # TODO: split the data points into good configurations, top N percent and the bad configurations\n",
    "    \n",
    "    # TODO: refit KDE for the current budget\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run BOHB (Bayesian optimization and Hyperband) where, given enough data, we select new configurations based on our model. After running BOHB we can plot again the incumbent trajectory and visualize for each configuration whether it was sampled randomly or from our model. You should see configuration suggested by our model are on average better than random configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 40  # maximum iterations/epochs per configuration\n",
    "eta = 3 # defines downsampling rate (default=3)\n",
    "s_max = int(np.log(max_iter)/np.log(eta))  # number of unique executions of Successive Halving (minus one)\n",
    "B = (s_max+1)*max_iter  # total number of iterations (without reuse) per execution of successive halving (n,r)\n",
    "\n",
    "# for visualization purposes only\n",
    "runtime_trajectory = []\n",
    "loss_over_time = []\n",
    "sampled_from_model = []\n",
    "current_time = 0\n",
    "\n",
    "# dictionaries for bookkeeping: the keys will be the corresponding budgets\n",
    "configs = dict()\n",
    "losses = dict()\n",
    "kde_models = dict()\n",
    "\n",
    "# Run K brackets of successive halving with different n and r\n",
    "for k in range(12):\n",
    "\n",
    "    s = int(np.arange(s_max + 1)[::-1][k % (s_max+1)])\n",
    "    n = int(np.ceil(int(B/max_iter/(s+1))*eta**s)) # initial number of configurations\n",
    "    r = max_iter*eta**(-s) # initial number of iterations to run configurations for\n",
    "\n",
    "    for i in range(s+1):\n",
    "\n",
    "        # Run each of the n_i configs for r_i iterations and keep best n_i/eta\n",
    "        n_i = n*eta**(-i)\n",
    "        r_i = r*eta**(i)\n",
    "        val_losses = []\n",
    "        T = []\n",
    "        for j in range(n):\n",
    "            x_new, from_model = sample()\n",
    "            y_new = objective_function(x_new, epoch=r_i)        \n",
    "            \n",
    "            c_new = runtime(x_new, epoch=r_i)      \n",
    "            \n",
    "            current_time += c_new\n",
    "            sampled_from_model.append(from_model)\n",
    "            runtime_trajectory.append(current_time)\n",
    "            loss_over_time.append(y_new)\n",
    "            \n",
    "            val_losses.append(y_new)\n",
    "            T.append(x_new)\n",
    "            update(r_i, x_new, y_new)\n",
    "            plt.scatter(np.ones(len(val_losses)) * r_i, val_losses)\n",
    "\n",
    "\n",
    "            \n",
    "        T = [ T[i] for i in np.argsort(val_losses)[0:int( n_i/eta )] ]\n",
    "        \n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"validation error\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"successive halving\")\n",
    "    plt.xlim(0, 41)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.array(runtime_trajectory)[sampled_from_model], np.array(loss_over_time)[sampled_from_model], label=\"model\")\n",
    "not_sampled_from_model = [not i for i in sampled_from_model]\n",
    "\n",
    "plt.scatter(np.array(runtime_trajectory)[not_sampled_from_model], np.array(loss_over_time)[not_sampled_from_model], label=\"random\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.ylabel(\"validation error\")\n",
    "plt.xlabel(\"runtime (seconds)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
